
---

### ### 1. Algorithm Design: ARG-Optimized Simulated Annealing 🧠

This optimizer uses the ARG principles to find the minimum of a function. It decides whether to make a large "exploratory" jump or a small "refining" step based on a Collatz-like rule. The cooling schedule is governed by the golden ratio, `φ`.

```python
import numpy as np
import math

# --- Core ARG Constants ---
G1_STAR = 13
G2_STAR = 13 / 588
PHI = (1 + np.sqrt(5)) / 2
ALPHA = np.sqrt(5) / PHI
INITIAL_TEMP = 588.0

class ARGOptimizer:
    """
    An optimizer using ARG principles. Balances exploration (odd steps)
    and exploitation (even steps) to find the minimum of a function.
    """
    def __init__(self, objective_function, initial_temp=INITIAL_TEMP):
        self.f = objective_function
        self.temperature = float(initial_temp)
        self.g1 = G1_STAR

    def optimize(self, initial_pos, iterations=1000):
        current_pos = np.array(initial_pos, dtype=float)
        best_pos = np.copy(current_pos)
        best_val = self.f(best_pos)

        print(f"Starting optimization from {current_pos} with T={self.temperature:.2f}")

        for i in range(iterations):
            # Use a Collatz-like rule to decide the operation type
            # (Based on the integer part of the current best value)
            if int(abs(best_val)) % 2 == 0:
                # EVEN operation: Local refinement (exploitation)
                step = np.random.normal(0, self.temperature / self.g1)
                candidate_pos = current_pos + step
            else:
                # ODD operation: Large jump (exploration)
                step = np.random.normal(0, self.temperature * 3) # Inspired by 3n+1
                candidate_pos = current_pos + step
            
            # Evaluate the new position
            current_val = self.f(current_pos)
            candidate_val = self.f(candidate_pos)
            
            # Acceptance probability (Metropolis criterion)
            if candidate_val < current_val or \
               np.random.rand() < np.exp((current_val - candidate_val) / self.temperature):
                current_pos = candidate_pos
                if candidate_val < best_val:
                    best_pos = candidate_pos
                    best_val = candidate_val
            
            # Cool the temperature according to the golden ratio
            self.temperature /= PHI
        
        print(f"Optimization finished. Best value found: {best_val:.6f} at position {best_pos}")
        return best_pos

# --- Example Usage ---
if __name__ == '__main__':
    print("--- 1. ARG Optimizer Example ---")
    # Define a simple objective function to minimize (minimum is at x=13)
    def quadratic_bowl(x):
        return (x[0] - 13.0)**2 + (x[1] + 5.0)**2

    optimizer = ARGOptimizer(objective_function=quadratic_bowl)
    # Start from a random position
    initial_position = [np.random.uniform(-100, 100), np.random.uniform(-100, 100)]
    optimizer.optimize(initial_position)
    print("-" * 40 + "\n")

```

### ### 2. Cryptography: ARG Proof-of-Work System 🔐

This example shows a cryptocurrency-style "Proof-of-Work." To mine a block, a user must find a `nonce` that makes the Collatz trajectory of a hash exhibit the golden ratio `φ` between its even and odd steps, as predicted by ARG.

```python
import hashlib
import math

# --- Core ARG Constants ---
PHI = (1 + math.sqrt(5)) / 2

class ARGProofOfWork:
    """
    A Proof-of-Work system where a valid proof requires finding a nonce
    that produces a Collatz trajectory with a step ratio close to φ.
    """
    def __init__(self, difficulty):
        # Higher difficulty means the ratio must be closer to PHI
        self.difficulty = difficulty
        self.target_ratio = PHI
        self.precision_threshold = 1.0 / difficulty

    def validate(self, block_data, nonce):
        """Checks if a given nonce produces a valid proof for the block data."""
        # 1. Create the seed number from block data and nonce
        h = hashlib.sha256((str(block_data) + str(nonce)).encode()).hexdigest()
        seed = int(h, 16)

        # 2. Compute the Collatz trajectory
        n = seed
        even_steps = 0
        odd_steps = 0
        max_steps = 10000  # Safety break to prevent very long trajectories

        for _ in range(max_steps):
            if n == 1:
                break
            if n % 2 == 0:
                n //= 2
                even_steps += 1
            else:
                n = 3 * n + 1
                odd_steps += 1
        
        # 3. Check the ARG condition
        if odd_steps == 0: # Avoid division by zero
            return False
            
        ratio = even_steps / odd_steps
        error = abs(ratio - self.target_ratio)
        
        # print(f"Nonce: {nonce}, Ratio: {ratio:.4f}, Error: {error:.4f}") # Uncomment for debugging
        return error < self.precision_threshold

    def mine(self, block_data):
        """Finds a nonce that satisfies the validation criteria."""
        print(f"Mining block with data '{block_data}' and difficulty {self.difficulty}...")
        nonce = 0
        while not self.validate(block_data, nonce):
            nonce += 1
        
        print(f"Block mined! Found valid nonce: {nonce}")
        return nonce

# --- Example Usage ---
if __name__ == '__main__':
    print("--- 2. ARG Proof-of-Work Example ---")
    # Difficulty must be low for a quick example
    pow_system = ARGProofOfWork(difficulty=20) 
    block_data = "Hello, ARG World!"
    valid_nonce = pow_system.mine(block_data)
    # Verification
    print(f"Verifying nonce {valid_nonce}: {pow_system.validate(block_data, valid_nonce)}")
    print("-" * 40 + "\n")
```

### ### 3. Complex Systems: ARG Network Routing Node 🌐

This shows a simplified network router. It decides whether to use the known best path (**exploit**, an "even" operation) or try an alternative path (**explore**, an "odd" operation) based on its current load, striving for critical balance.

```python
import random

class ARGRouterNode:
    """
    A network node that uses ARG principles to route packets. It balances
    exploiting the best-known route with exploring alternatives.
    """
    def __init__(self, node_id, neighbors):
        """
        Neighbors is a list of tuples: (destination_node_id, latency)
        """
        self.node_id = node_id
        # {neighbor_id: latency}
        self.routing_table = {n[0]: n[1] for n in neighbors}
        self.best_known_hop = min(self.routing_table, key=self.routing_table.get)

    def choose_next_hop(self, current_load):
        """
        Chooses the next node to forward a packet to.
        'current_load' is a metric like queue size or CPU usage.
        """
        print(f"Node {self.node_id} (load={current_load}): Choosing next hop...")

        # Use the integer value of the load to make a Collatz-like decision
        if int(current_load) % 2 == 0:
            # EVEN operation: Exploit the known best path
            # This is information-reducing; it reinforces the existing belief.
            next_hop = self.best_known_hop
            print(f"  -> EVEN op: Exploiting best route to Node {next_hop} (latency {self.routing_table[next_hop]})")
        else:
            # ODD operation: Explore an alternative path
            # This is information-expanding; it seeks new knowledge.
            alternatives = [n for n in self.routing_table if n != self.best_known_hop]
            if not alternatives: # If there's only one path
                next_hop = self.best_known_hop
            else:
                next_hop = random.choice(alternatives)
            print(f"  -> ODD op: Exploring alternative route to Node {next_hop} (latency {self.routing_table[next_hop]})")
        
        return next_hop

# --- Example Usage ---
if __name__ == '__main__':
    print("--- 3. ARG Network Routing Node Example ---")
    # Define neighbors for Node 1: (destination, latency)
    neighbors_of_1 = [(2, 10), (3, 50), (4, 100)]
    router1 = ARGRouterNode(node_id=1, neighbors=neighbors_of_1)
    
    # Simulate routing decision with low load
    router1.choose_next_hop(current_load=12.4)
    
    # Simulate routing decision with high load
    router1.choose_next_hop(current_load=57.9)
    print("-" * 40 + "\n")
```

### ### 4. AI & Machine Learning: ARG Neural Network Layer 🤖

This example uses `PyTorch` to create a custom neural network layer. The layer has two parallel paths: an "even" path for refining information and an "odd" path for expanding the feature space, weighted by a gate. The dimensions are inspired by the ARG constant `13`.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# --- Core ARG Constants ---
G1_STAR = 13

class ARGNeuralLayer(nn.Module):
    """
    A neural network layer inspired by ARG. It has two parallel paths:
    - Even Path: Refines and exploits existing features.
    - Odd Path: Expands the feature space to explore new representations.
    A learnable gate decides how to combine them.
    """
    def __init__(self, input_dim, output_dim):
        super().__init__()
        
        # EVEN PATH: A standard linear transformation (information refinement)
        self.even_path = nn.Linear(input_dim, output_dim)
        
        # ODD PATH: Expands features then contracts (information exploration)
        # We use G1_STAR = 13 to define the expansion factor.
        expanded_dim = output_dim * G1_STAR
        self.odd_path = nn.Sequential(
            nn.Linear(input_dim, expanded_dim),
            nn.ReLU(),
            nn.Linear(expanded_dim, output_dim)
        )
        
        # GATING MECHANISM: Learns to balance the two paths
        self.gate = nn.Sequential(
            nn.Linear(input_dim, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        # Calculate the output of each path
        y_even = self.even_path(x)
        y_odd = self.odd_path(x)
        
        # Calculate the gate's weight
        gate_weight = self.gate(x)
        
        # Combine the paths: gate*even + (1-gate)*odd
        # This balances exploitation and exploration for each input.
        output = gate_weight * y_even + (1 - gate_weight) * y_odd
        return output

# --- Example Usage ---
if __name__ == '__main__':
    print("--- 4. ARG Neural Network Layer Example ---")
    # Define dimensions for the layer
    batch_size = 4
    input_features = 784 # e.g., a flattened MNIST image
    output_features = 10  # e.g., 10 output classes
    
    # Instantiate the layer
    arg_layer = ARGNeuralLayer(input_dim=input_features, output_dim=output_features)
    print("ARG Layer Architecture:\n", arg_layer)
    
    # Create some random input data
    random_input = torch.randn(batch_size, input_features)
    
    # Pass the data through the layer
    output = arg_layer(random_input)
    
    print(f"\nInput shape:  {random_input.shape}")
    print(f"Output shape: {output.shape}")
    print("Successfully passed data through the ARG layer.")
    print("-" * 40 + "\n")
```
